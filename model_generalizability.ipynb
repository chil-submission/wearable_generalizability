{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b2703d0-5b9d-4f32-b00e-1c6c229e7681",
   "metadata": {},
   "source": [
    "# DISDE Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26ab556d-b057-4014-a866-31e1ec8a5055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphviz in /home/patrick/miniconda3/lib/python3.8/site-packages (0.20.1)\n",
      "\u001b[33mDEPRECATION: pytorch-lightning 1.7.7 has a non-standard dependency specifier torch>=1.9.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: tsfresh 0.18.0 has a non-standard dependency specifier matrixprofile>=1.1.10<2.0.0. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of tsfresh or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Found existing installation: numpy 1.24.4\n",
      "Uninstalling numpy-1.24.4:\n",
      "  Successfully uninstalled numpy-1.24.4\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Downloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[33mDEPRECATION: pytorch-lightning 1.7.7 has a non-standard dependency specifier torch>=1.9.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: tsfresh 0.18.0 has a non-standard dependency specifier matrixprofile>=1.1.10<2.0.0. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of tsfresh or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: numpy\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "manim 0.15.2 requires decorator<6.0.0,>=5.0.7, but you have decorator 4.4.2 which is incompatible.\n",
      "matrixprofile 1.1.10 requires protobuf==3.11.2, but you have protobuf 4.25.1 which is incompatible.\n",
      "scikit-survival 0.15.0.post0 requires scikit-learn<0.25,>=0.24.0, but you have scikit-learn 1.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.24.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: xgboost in /home/patrick/miniconda3/lib/python3.8/site-packages (2.0.3)\n",
      "Requirement already satisfied: numpy in /home/patrick/miniconda3/lib/python3.8/site-packages (from xgboost) (1.24.4)\n",
      "Requirement already satisfied: scipy in /home/patrick/miniconda3/lib/python3.8/site-packages (from xgboost) (1.10.1)\n",
      "\u001b[33mDEPRECATION: pytorch-lightning 1.7.7 has a non-standard dependency specifier torch>=1.9.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: tsfresh 0.18.0 has a non-standard dependency specifier matrixprofile>=1.1.10<2.0.0. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of tsfresh or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: fairlearn in /home/patrick/miniconda3/lib/python3.8/site-packages (0.10.0)\n",
      "Requirement already satisfied: numpy>=1.24.4 in /home/patrick/miniconda3/lib/python3.8/site-packages (from fairlearn) (1.24.4)\n",
      "Requirement already satisfied: pandas>=2.0.3 in /home/patrick/miniconda3/lib/python3.8/site-packages (from fairlearn) (2.0.3)\n",
      "Requirement already satisfied: scikit-learn>=1.2.1 in /home/patrick/miniconda3/lib/python3.8/site-packages (from fairlearn) (1.3.1)\n",
      "Requirement already satisfied: scipy>=1.9.3 in /home/patrick/miniconda3/lib/python3.8/site-packages (from fairlearn) (1.10.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/patrick/miniconda3/lib/python3.8/site-packages (from pandas>=2.0.3->fairlearn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/patrick/miniconda3/lib/python3.8/site-packages (from pandas>=2.0.3->fairlearn) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/patrick/miniconda3/lib/python3.8/site-packages (from pandas>=2.0.3->fairlearn) (2022.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/patrick/miniconda3/lib/python3.8/site-packages (from scikit-learn>=1.2.1->fairlearn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/patrick/miniconda3/lib/python3.8/site-packages (from scikit-learn>=1.2.1->fairlearn) (3.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/patrick/miniconda3/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas>=2.0.3->fairlearn) (1.16.0)\n",
      "\u001b[33mDEPRECATION: pytorch-lightning 1.7.7 has a non-standard dependency specifier torch>=1.9.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: tsfresh 0.18.0 has a non-standard dependency specifier matrixprofile>=1.1.10<2.0.0. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of tsfresh or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: lightgbm in /home/patrick/miniconda3/lib/python3.8/site-packages (4.3.0)\n",
      "Requirement already satisfied: numpy in /home/patrick/miniconda3/lib/python3.8/site-packages (from lightgbm) (1.24.4)\n",
      "Requirement already satisfied: scipy in /home/patrick/miniconda3/lib/python3.8/site-packages (from lightgbm) (1.10.1)\n",
      "\u001b[33mDEPRECATION: pytorch-lightning 1.7.7 has a non-standard dependency specifier torch>=1.9.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: tsfresh 0.18.0 has a non-standard dependency specifier matrixprofile>=1.1.10<2.0.0. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of tsfresh or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: pandas in /home/patrick/miniconda3/lib/python3.8/site-packages (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/patrick/miniconda3/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/patrick/miniconda3/lib/python3.8/site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/patrick/miniconda3/lib/python3.8/site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /home/patrick/miniconda3/lib/python3.8/site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: six>=1.5 in /home/patrick/miniconda3/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\u001b[33mDEPRECATION: pytorch-lightning 1.7.7 has a non-standard dependency specifier torch>=1.9.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: tsfresh 0.18.0 has a non-standard dependency specifier matrixprofile>=1.1.10<2.0.0. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of tsfresh or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting dask>2023.3.2\n",
      "  Downloading dask-2023.5.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting distributed>2023.3.2\n",
      "  Downloading distributed-2023.5.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: click>=8.0 in /home/patrick/miniconda3/lib/python3.8/site-packages (from dask>2023.3.2) (8.1.2)\n",
      "Requirement already satisfied: cloudpickle>=1.5.0 in /home/patrick/miniconda3/lib/python3.8/site-packages (from dask>2023.3.2) (2.0.0)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /home/patrick/miniconda3/lib/python3.8/site-packages (from dask>2023.3.2) (2022.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/patrick/miniconda3/lib/python3.8/site-packages (from dask>2023.3.2) (21.3)\n",
      "Requirement already satisfied: partd>=1.2.0 in /home/patrick/miniconda3/lib/python3.8/site-packages (from dask>2023.3.2) (1.2.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /home/patrick/miniconda3/lib/python3.8/site-packages (from dask>2023.3.2) (5.4.1)\n",
      "Requirement already satisfied: toolz>=0.10.0 in /home/patrick/miniconda3/lib/python3.8/site-packages (from dask>2023.3.2) (0.11.2)\n",
      "Collecting importlib-metadata>=4.13.0 (from dask>2023.3.2)\n",
      "  Downloading importlib_metadata-7.0.1-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: jinja2>=2.10.3 in /home/patrick/miniconda3/lib/python3.8/site-packages (from distributed>2023.3.2) (2.10.3)\n",
      "Collecting locket>=1.0.0 (from distributed>2023.3.2)\n",
      "  Downloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\n",
      "Requirement already satisfied: msgpack>=1.0.0 in /home/patrick/miniconda3/lib/python3.8/site-packages (from distributed>2023.3.2) (1.0.3)\n",
      "Requirement already satisfied: psutil>=5.7.0 in /home/patrick/miniconda3/lib/python3.8/site-packages (from distributed>2023.3.2) (5.9.1)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.5 in /home/patrick/miniconda3/lib/python3.8/site-packages (from distributed>2023.3.2) (2.4.0)\n",
      "Requirement already satisfied: tblib>=1.6.0 in /home/patrick/miniconda3/lib/python3.8/site-packages (from distributed>2023.3.2) (1.7.0)\n",
      "Requirement already satisfied: tornado>=6.0.3 in /home/patrick/miniconda3/lib/python3.8/site-packages (from distributed>2023.3.2) (6.1)\n",
      "Requirement already satisfied: urllib3>=1.24.3 in /home/patrick/miniconda3/lib/python3.8/site-packages (from distributed>2023.3.2) (1.26.9)\n",
      "Collecting zict>=2.2.0 (from distributed>2023.3.2)\n",
      "  Downloading zict-3.0.0-py2.py3-none-any.whl (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.3/43.3 kB\u001b[0m \u001b[31m646.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /home/patrick/miniconda3/lib/python3.8/site-packages (from importlib-metadata>=4.13.0->dask>2023.3.2) (3.8.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/patrick/miniconda3/lib/python3.8/site-packages (from jinja2>=2.10.3->distributed>2023.3.2) (2.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/patrick/miniconda3/lib/python3.8/site-packages (from packaging>=20.0->dask>2023.3.2) (3.0.9)\n",
      "Downloading dask-2023.5.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading distributed-2023.5.0-py3-none-any.whl (966 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m966.6/966.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading importlib_metadata-7.0.1-py3-none-any.whl (23 kB)\n",
      "\u001b[33mDEPRECATION: pytorch-lightning 1.7.7 has a non-standard dependency specifier torch>=1.9.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: tsfresh 0.18.0 has a non-standard dependency specifier matrixprofile>=1.1.10<2.0.0. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of tsfresh or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: zict, locket, importlib-metadata, dask, distributed\n",
      "  Attempting uninstall: zict\n",
      "    Found existing installation: zict 2.1.0\n",
      "    Uninstalling zict-2.1.0:\n",
      "      Successfully uninstalled zict-2.1.0\n",
      "  Attempting uninstall: locket\n",
      "    Found existing installation: locket 0.2.0\n",
      "    Uninstalling locket-0.2.0:\n",
      "      Successfully uninstalled locket-0.2.0\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 4.12.0\n",
      "    Uninstalling importlib-metadata-4.12.0:\n",
      "      Successfully uninstalled importlib-metadata-4.12.0\n",
      "  Attempting uninstall: dask\n",
      "    Found existing installation: dask 2022.1.0\n",
      "    Uninstalling dask-2022.1.0:\n",
      "      Successfully uninstalled dask-2022.1.0\n",
      "  Attempting uninstall: distributed\n",
      "    Found existing installation: distributed 2022.1.0\n",
      "    Uninstalling distributed-2022.1.0:\n",
      "      Successfully uninstalled distributed-2022.1.0\n",
      "Successfully installed dask-2023.5.0 distributed-2023.5.0 importlib-metadata-7.0.1 locket-1.0.0 zict-3.0.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Note, WhyShift uses some odd, very specific versions of certain packages. This may need futher configuration\n",
    "!pip install graphviz\n",
    "!pip uninstall -y numpy\n",
    "!pip install --no-cache-dir numpy\n",
    "!pip install xgboost\n",
    "!pip install fairlearn\n",
    "!pip install --no-cache-dir lightgbm\n",
    "!pip install --upgrade pandas\n",
    "!pip install \\\n",
    "    'dask>2023.3.2' \\\n",
    "    'distributed>2023.3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8b0e005-993d-4160-b7fb-c28cb84d8547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "sys.path.insert(0, './')\n",
    "\n",
    "from whyshift import degradation_decomp, fetch_model, risk_region\n",
    "import torch \n",
    "import random\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, average_precision_score, roc_auc_score, accuracy_score\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e367759d-4b2b-48e5-a033-c26191e40fce",
   "metadata": {},
   "source": [
    "# WhyShift Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b05a8eda-81bd-4461-9ebf-7d6e2c679543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the following to the WhyShift package\n",
    "\n",
    "model = fetch_model('hgbc')\n",
    "#model.fit(source_X_train, source_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde69645-4899-4336-8135-6d5c798e658c",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42668b1f-64b0-42a2-9165-5fcefbed025a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_daily_data(df, columns, pid_name, date_name):\n",
    "    \"\"\"\n",
    "    df: the dataframe with samples\n",
    "    columns: should be a dictionary, with the columns to filter and the filter values, i.e. {'resting_heart_rate': (20,200)}\n",
    "    pid_name: name of the pid column\n",
    "    date_name: name of the date column, assumed not to already be datetime based\n",
    "    \"\"\"\n",
    "\n",
    "    df['pid'] = df[pid_name]\n",
    "    # Make sure dates are in pandas datetime\n",
    "    df['date'] = pd.to_datetime(df[date_name])\n",
    "    \n",
    "    # Set a multi-index on PID and date\n",
    "    df = df.set_index(['pid', 'date']).sort_index()\n",
    "\n",
    "    # Iterate through the set of columns to filter on\n",
    "    for key, values in columns.items():\n",
    "        df = df[df[key].between(values[0], values[1])]\n",
    "        \n",
    "    # Resample to daily, important for lagged-z-scoring\n",
    "    df = df.groupby('pid').apply(lambda x: x.droplevel(0).resample('D').asfreq())\n",
    "\n",
    "    df['day_of_week'] = df.index.get_level_values(1).dayofweek\n",
    "    \n",
    "    return df\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1e1869a-187c-4dd3-8a54-3b443315ce8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "homekit_nightly = pd.read_csv('/homekit2020neurips/fitbit_day_level_activity.csv', index_col=0)\n",
    "\n",
    "columns = {\n",
    "    'resting_heart_rate': (20, 200),\n",
    "    'caloriesOut': (500, 20000),\n",
    "    'total_asleep_minutes': (60, 16*60*60), #Let's say 16 hours\n",
    "}\n",
    "\n",
    "homekit_nightly = clean_daily_data(homekit_nightly, columns, 'participant_id', 'date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7121a7e-27c2-44b9-b20c-1a46c0abf964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COVID-RED\n",
    "covid_red_nightly = pd.read_csv('./data/wd_20230515.csv')\n",
    "\n",
    "columns = {\n",
    "    'WDTEMP': (0, 45),\n",
    "    'WDPULSE': (20, 200),\n",
    "    'WDSLEEP': (60, 16*60*60), #Let's say 16 hours\n",
    "}\n",
    "\n",
    "covid_red_nightly = clean_daily_data(covid_red_nightly, columns, 'SUBJID', 'WDDT')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae48ea8e-0afd-4afc-95b3-5e3cb51f5eb7",
   "metadata": {},
   "source": [
    "# Surveys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e311d3e7-0f0f-4a6b-8c68-1ec0d833e545",
   "metadata": {},
   "source": [
    "## Homekit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d67ce21-7da2-41d4-b2b3-4e7c3b16855f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "homekit_surveys = pd.read_csv('/homekit2020neurips/daily_surveys_onehot.csv')\n",
    "homekit_surveys['date'] = pd.to_datetime(homekit_surveys['timestamp'].apply(lambda x: x[:10]))\n",
    "homekit_surveys = homekit_surveys.drop_duplicates(subset=['participant_id','date'], keep = 'last')\n",
    "homekit_surveys = homekit_surveys.set_index(['participant_id', 'date'])\n",
    "homekit_surveys = homekit_surveys.sort_index()\n",
    "\n",
    "indeces = list(set(homekit_nightly.index).intersection(homekit_surveys.index))\n",
    "homekit_nightly[homekit_surveys.columns] = np.nan\n",
    "homekit_nightly.loc[indeces, homekit_surveys.columns] = homekit_surveys.loc[indeces].values\n",
    "\n",
    "\"\"\"\n",
    "Fever labels\n",
    "\"\"\"\n",
    "homekit_nightly['sx_Fever'] = homekit_nightly[['symptom_severity__fever_q_2', 'symptom_severity__fever_q_3']].sum(axis = 1)\n",
    "\n",
    "\"\"\"\n",
    "Flu labels: fever and one of: sore throat or cough\n",
    "\"\"\"\n",
    "# Body ache\n",
    "homekit_nightly['sx_Fever_ILI'] = homekit_nightly[['symptom_severity__fever_q_1','symptom_severity__fever_q_2', 'symptom_severity__fever_q_3']].sum(axis = 1)\n",
    "homekit_nightly['sx_Sore_throat_ILI'] = homekit_nightly[['symptom_severity__q_sore_throat_1','symptom_severity__q_sore_throat_2', 'symptom_severity__q_sore_throat_3']].sum(axis = 1)\n",
    "homekit_nightly['sx_Cough_ILI'] = homekit_nightly[['symptom_severity__cough_q_1','symptom_severity__cough_q_2', 'symptom_severity__cough_q_2']].sum(axis = 1)\n",
    "homekit_nightly['ili_sum'] = homekit_nightly['sx_Fever_ILI'] + homekit_nightly[['sx_Sore_throat_ILI', 'sx_Cough_ILI']].max(axis = 1)\n",
    "\n",
    "homekit_nightly['sx_ILI'] = np.nan\n",
    "\n",
    "homekit_nightly.loc[homekit_nightly[homekit_nightly['ili_sum'] == 2].index, 'sx_ILI'] = 1\n",
    "homekit_nightly.loc[homekit_nightly[homekit_nightly['ili_sum'] == 1].index, 'sx_ILI'] = 0\n",
    "homekit_nightly.loc[homekit_nightly[homekit_nightly['ili_sum'] == 0].index, 'sx_ILI'] = 0\n",
    "homekit_nightly['have_flu'] = homekit_nightly['sx_ILI']\n",
    "\n",
    "\"\"\"\n",
    "Viral positivity label\n",
    "\"\"\"\n",
    "hk_lab_tests = pd.read_csv('/homekit2020neurips/lab_results_with_triggerdate.csv', index_col = 0)\n",
    "hk_lab_tests['date'] = pd.to_datetime(pd.to_datetime(hk_lab_tests['trigger_datetime']).dt.date)\n",
    "\n",
    "flus = [\"Influenza A (Flu A)\",\"Influenza B (Flu B)\"]\n",
    "hk_lab_tests = hk_lab_tests.drop_duplicates(subset=['participant_id','date'], keep = 'last')\n",
    "hk_lab_tests = hk_lab_tests.set_index(['participant_id', 'date'])\n",
    "hk_lab_tests['type'] = hk_lab_tests['result'].apply(lambda x: type(x))\n",
    "hk_lab_tests = hk_lab_tests[hk_lab_tests['type'] == type('dummy_string')]\n",
    "hk_lab_tests = hk_lab_tests.drop('first_report_yn', axis = 1) \n",
    "\n",
    "indeces = list(set(homekit_nightly.index).intersection(hk_lab_tests.index))\n",
    "homekit_nightly[hk_lab_tests.columns] = np.nan\n",
    "homekit_nightly.loc[indeces, hk_lab_tests.columns] = hk_lab_tests.loc[indeces].values\n",
    "homekit_nightly['is_pos'] = 0\n",
    "is_pos_index = homekit_nightly[(homekit_nightly['test_name'].isin(flus)) & (homekit_nightly['result'] == 'Detected')].index\n",
    "homekit_nightly.loc[is_pos_index, 'is_pos'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6160a7-f80c-4b9d-a8d0-26c4583eefd5",
   "metadata": {},
   "source": [
    "# COVID-RED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28bffe33-1960-47f8-83b7-695c671a3be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Viral positivity\n",
    "\"\"\"\n",
    "covid_red_nightly['is_pos'] = 0\n",
    "covid_red_nightly.loc[covid_red_nightly[covid_red_nightly['WDDIAG'] == 'positive'].index, 'is_pos'] = 1\n",
    "\n",
    "\"\"\"\n",
    "Fever sypmtoms\n",
    "\"\"\"\n",
    "covid_red_nightly['sx_Fever'] = covid_red_nightly['WDSYMP'].astype(str).apply(lambda x: np.nan if 'nan' in x else (1 if 'fever' in x else 0))\n",
    "covid_red_nightly['sx_Cough'] = covid_red_nightly['WDSYMP'].astype(str).apply(lambda x: np.nan if 'nan' in x else (1 if 'cough' in x else 0))\n",
    "covid_red_nightly['sx_Sore_throat'] = covid_red_nightly['WDSYMP'].astype(str).apply(lambda x: np.nan if 'nan' in x else (1 if 'sore_throat' in x else 0))\n",
    "\n",
    "\"\"\"\n",
    "Flu sypmtoms\n",
    "\"\"\"\n",
    "covid_red_nightly['ili_sum'] = covid_red_nightly['sx_Fever'] + covid_red_nightly[['sx_Cough', 'sx_Sore_throat']].max(axis = 1)\n",
    "covid_red_nightly['have_flu'] = np.nan\n",
    "\n",
    "covid_red_nightly.loc[covid_red_nightly[covid_red_nightly['ili_sum'] == 2].index, 'have_flu'] = 1\n",
    "covid_red_nightly.loc[covid_red_nightly[covid_red_nightly['ili_sum'] == 1].index, 'have_flu'] = 0\n",
    "covid_red_nightly.loc[covid_red_nightly[covid_red_nightly['ili_sum'] == 0].index, 'have_flu'] = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb466302-15e7-45aa-8012-860640f0cb00",
   "metadata": {},
   "source": [
    "# Util Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ec6d60c-b29a-4303-9ec7-aba5f55a4a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score_modalities(df, modalities, length, min_num, offset, pid_name):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    df: a pandas dataframe, multi-indexed by pid and date\n",
    "    modalities: iterable of strings, should be the names of columns\n",
    "    length: the length of the window to z-score by\n",
    "    min_num: the minumum number of non-missing days in the baseline period\n",
    "    offset: how many days lagged to start the baseline period\n",
    "    pid_name: the name of the index level with the pids\n",
    "    \"\"\"\n",
    "    \n",
    "    grouped_mean = df.groupby(pid_name)[modalities].rolling(window = length, min_periods = min_num).mean().shift(offset)\n",
    "    grouped_std = df.groupby(pid_name)[modalities].rolling(window = length, min_periods = min_num).std().shift(offset)\n",
    "\n",
    "    z_modality_names = [mod + \"_z\" for mod in modalities]\n",
    "    df[z_modality_names] = (df[modalities].values - grouped_mean.values)/grouped_std.values\n",
    "    return df\n",
    "\n",
    "def get_ML_dataset(z_scored_df, modalities, prediction_or_detection, pid_name, ground_truth_column):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    z_scored_df: a pandas dataframe, multi-indexed by pid and date. Note, since this will be using shift, it should be complete in time, with nan's for any in-between dates without data\n",
    "        - also note, dates should correspond to wearable data from the NIGHT BEFORE\n",
    "        - Wearable data should already be z-scored by relevent baseline period\n",
    "    modalities: iterable of strings, should be the names of columns\n",
    "    prediction_or_detection: if prediction, take the data from 2 nights and the night before the ground truth day\n",
    "        - if detection, take data from the night before and the night after detection\n",
    "    pid_name: the name of the index level with the pids\n",
    "    \"\"\"\n",
    "    z_scored_df['day_of_week'] = z_scored_df.index.get_level_values(1).dayofweek\n",
    "    z_scored_df[np.arange(0,7).astype(str)] = pd.get_dummies(z_scored_df['day_of_week']).astype(int)\n",
    "    \n",
    "    if prediction_or_detection == 'prediction':\n",
    "                \n",
    "        shift = 2\n",
    "        # Assign the correct values to two nights before\n",
    "        three_before_names = [f\"{mod}_3_nights_before\" for mod in modalities]\n",
    "        z_scored_df[three_before_names] = z_scored_df.groupby(pid_name).shift(shift)[modalities].values\n",
    "        \n",
    "        shift = 1\n",
    "\n",
    "        # Assign the correct values to two nights before\n",
    "        two_before_names = [f\"{mod}_2_nights_before\" for mod in modalities]\n",
    "        z_scored_df[two_before_names] = z_scored_df.groupby(pid_name).shift(shift)[modalities].values\n",
    "        \n",
    "        # Assign the correct values to the night before columns\n",
    "        one_before_names = [f\"{mod}_1_night_before\" for mod in modalities]\n",
    "        z_scored_df[one_before_names] = z_scored_df[modalities].values      \n",
    "        \n",
    "\n",
    "        return z_scored_df[np.hstack([three_before_names, two_before_names, one_before_names, np.arange(0,7).astype(str), ground_truth_column])], np.hstack([three_before_names, two_before_names, one_before_names, np.arange(0,7).astype(str)])\n",
    "        \n",
    "    elif prediction_or_detection == 'detection':\n",
    "\n",
    "        shift = 1\n",
    "\n",
    "        # Assign the correct values to two nights before\n",
    "        two_before_names = [f\"{mod}_2_nights_before\" for mod in modalities]\n",
    "        z_scored_df[two_before_names] = z_scored_df.groupby(pid_name).shift(shift)[modalities].values\n",
    "        \n",
    "        shift = -1\n",
    "\n",
    "        # Assign the correct values to two nights before\n",
    "        night_after_names = [f\"{mod}_night_after\" for mod in modalities]\n",
    "        z_scored_df[night_after_names] = z_scored_df.groupby(pid_name).shift(shift)[modalities].values\n",
    "        \n",
    "        # Assign the correct values to the night before columns\n",
    "        night_before_names = [f\"{mod}_night_before\" for mod in modalities]\n",
    "        z_scored_df[night_before_names] = z_scored_df[modalities].values\n",
    "\n",
    "        return z_scored_df[np.hstack([two_before_names, night_before_names, night_after_names, np.arange(0,7).astype(str), ground_truth_column])], np.hstack([two_before_names, night_before_names, night_after_names, np.arange(0,7).astype(str)])\n",
    "\n",
    "def calculate_sample_weights(y):\n",
    "    class_counts = np.bincount(y)\n",
    "    # Calculate the total number of samples\n",
    "    total_samples = len(y)\n",
    "    \n",
    "    # Calculate the proportion of each class\n",
    "    class_proportions = class_counts / total_samples\n",
    "    \n",
    "    # Assign weights inversely proportional to the class proportions\n",
    "    weights = (1.0 / class_proportions[y])/total_samples\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37959b1b-09c9-4423-be4b-149eff4fa98e",
   "metadata": {},
   "source": [
    "# Prediction, detection, all possible features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a7a535-27f1-4ab7-8499-f4f9cb68b80b",
   "metadata": {},
   "source": [
    "# Homekit2020 Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40063f16-8fb7-4bbb-838b-19907bae5108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Task is is_pos\n",
      "Average ROC: 0.858, Average Precision: 0.002, Average accuracy: 0.5\n",
      "\n",
      "\n",
      "Task is have_flu\n",
      "Average ROC: 0.637, Average Precision: 0.0159, Average accuracy: 0.5\n",
      "\n",
      "\n",
      "Task is sx_Fever\n",
      "Average ROC: 0.766, Average Precision: 0.0363, Average accuracy: 0.5083\n",
      "CPU times: user 4min 7s, sys: 4.16 s, total: 4min 11s\n",
      "Wall time: 38.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "optimal_offset = 12\n",
    "optimal_window_length = 10\n",
    "minimum_number = 6\n",
    "prediction_detection = 'prediction'\n",
    "\n",
    "\"\"\"\n",
    "Homekit dataset training\n",
    "\"\"\"\n",
    "tasks = ['is_pos', 'have_flu', 'sx_Fever']\n",
    "\n",
    "for ground_truth_label in tasks:\n",
    "    print(\"\\n\")\n",
    "    print(f\"Task is {ground_truth_label}\")\n",
    "\n",
    "    homekit_cols = ['resting_heart_rate','main_in_bed_minutes','main_efficiency','nap_count','total_asleep_minutes','total_in_bed_minutes','activityCalories','caloriesOut',\n",
    " 'caloriesBMR','marginalCalories','sedentaryMinutes','lightlyActiveMinutes','fairlyActiveMinutes','veryActiveMinutes']\n",
    "    \n",
    "    z_scored_df = z_score_modalities(homekit_nightly, homekit_cols, optimal_offset, minimum_number, optimal_window_length, 'pid')\n",
    "    z_score_columns = [col + \"_z\" for col in homekit_cols]\n",
    "    \n",
    "    dataset_df, feature_column_names = get_ML_dataset(z_scored_df, z_score_columns, prediction_detection, 'pid', ground_truth_label)\n",
    "    \n",
    "    dataset_df_drop = dataset_df[np.hstack([feature_column_names, [ground_truth_label]])].replace([np.inf, -np.inf, None], np.nan).dropna()\n",
    "    X, y = dataset_df_drop[feature_column_names].values, dataset_df_drop[ground_truth_label].values\n",
    "    \n",
    "    positive_ids = dataset_df_drop[dataset_df_drop[ground_truth_label] == 1].index.get_level_values(0).unique()\n",
    "    user_split_pids = pd.DataFrame(index = dataset_df_drop.index.get_level_values(0).unique(), columns = [ground_truth_label])\n",
    "    \n",
    "    # Dummy X for input to stratified K fold splitting\n",
    "    X = user_split_pids.values\n",
    "    \n",
    "    user_split_pids[ground_truth_label] = 0\n",
    "    user_split_pids.loc[positive_ids] = 1\n",
    "    \n",
    "    y = user_split_pids.values\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5, random_state = 42, shuffle=True)\n",
    "    skf.get_n_splits(X, y)\n",
    "    \n",
    "    accuracy_score_holder = []\n",
    "    au_roc_holder = []\n",
    "    av_prec_holder = []\n",
    "    \n",
    "    for fold, (train, test) in enumerate(skf.split(X, y)):\n",
    "        train_pids = user_split_pids.iloc[train].index\n",
    "        test_pids = user_split_pids.iloc[test].index\n",
    "    \n",
    "        X_train = dataset_df_drop.loc[train_pids, feature_column_names].values\n",
    "        X_test = dataset_df_drop.loc[test_pids, feature_column_names].values\n",
    "    \n",
    "        y_train = dataset_df_drop.loc[train_pids, ground_truth_label].values\n",
    "        y_test = dataset_df_drop.loc[test_pids, ground_truth_label].values\n",
    "    \n",
    "        clf = HistGradientBoostingClassifier(random_state = 42, l2_regularization = 0.2, early_stopping = False)\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        y_test_predict_prob = clf.predict_proba(X_test)\n",
    "        y_test_predict = clf.predict(X_test)\n",
    "\n",
    "        weights = calculate_sample_weights(y_test.astype(int))\n",
    "\n",
    "        accuracy_score_holder.append(accuracy_score(y_test, y_test_predict, sample_weight = weights))\n",
    "        au_roc_holder.append(roc_auc_score(y_test, y_test_predict_prob[:,1]))\n",
    "        av_prec_holder.append(average_precision_score(y_test, y_test_predict_prob[:,1]))\n",
    "    \n",
    "    print(f\"Average ROC: {np.array(au_roc_holder).mean().round(3)}, Average Precision: {np.array(av_prec_holder).mean().round(4)}, Average accuracy: {np.array(accuracy_score_holder).mean().round(4)}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec127c7-dc19-4a8d-8c85-ace3b4b919de",
   "metadata": {},
   "source": [
    "# Homekit2020 Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a23cd0f-7869-44eb-a1d5-4f623608c299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Task is is_pos\n",
      "Average ROC: 0.931, Average Precision: 0.0112, Average accuracy: 0.5\n",
      "\n",
      "\n",
      "Task is have_flu\n",
      "Average ROC: 0.638, Average Precision: 0.016, Average accuracy: 0.5006\n",
      "\n",
      "\n",
      "Task is sx_Fever\n",
      "Average ROC: 0.77, Average Precision: 0.0159, Average accuracy: 0.5\n",
      "CPU times: user 4min 18s, sys: 5.33 s, total: 4min 24s\n",
      "Wall time: 40.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "optimal_offset = 12\n",
    "optimal_window_length = 10\n",
    "minimum_number = 6\n",
    "prediction_detection = 'detection'\n",
    "\n",
    "\"\"\"\n",
    "Homekit dataset training\n",
    "\"\"\"\n",
    "tasks = ['is_pos', 'have_flu', 'sx_Fever']\n",
    "\n",
    "for ground_truth_label in tasks:\n",
    "    print(\"\\n\")\n",
    "    print(f\"Task is {ground_truth_label}\")\n",
    "\n",
    "    homekit_cols = ['resting_heart_rate','main_in_bed_minutes','main_efficiency','nap_count','total_asleep_minutes','total_in_bed_minutes','activityCalories','caloriesOut',\n",
    " 'caloriesBMR','marginalCalories','sedentaryMinutes','lightlyActiveMinutes','fairlyActiveMinutes','veryActiveMinutes']\n",
    "    \n",
    "    z_scored_df = z_score_modalities(homekit_nightly, homekit_cols, optimal_offset, minimum_number, optimal_window_length, 'pid')\n",
    "    z_score_columns = [col + \"_z\" for col in homekit_cols]\n",
    "    \n",
    "    dataset_df, feature_column_names = get_ML_dataset(z_scored_df, z_score_columns, prediction_detection, 'pid', ground_truth_label)\n",
    "    \n",
    "    dataset_df_drop = dataset_df[np.hstack([feature_column_names, [ground_truth_label]])].replace([np.inf, -np.inf, None], np.nan).dropna()\n",
    "    X, y = dataset_df_drop[feature_column_names].values, dataset_df_drop[ground_truth_label].values\n",
    "    \n",
    "    positive_ids = dataset_df_drop[dataset_df_drop[ground_truth_label] == 1].index.get_level_values(0).unique()\n",
    "    user_split_pids = pd.DataFrame(index = dataset_df_drop.index.get_level_values(0).unique(), columns = [ground_truth_label])\n",
    "    \n",
    "    # Dummy X for input to stratified K fold splitting\n",
    "    X = user_split_pids.values\n",
    "    \n",
    "    user_split_pids[ground_truth_label] = 0\n",
    "    user_split_pids.loc[positive_ids] = 1\n",
    "    \n",
    "    y = user_split_pids.values\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5, random_state = 42, shuffle=True)\n",
    "    skf.get_n_splits(X, y)\n",
    "    \n",
    "    accuracy_score_holder = []\n",
    "    au_roc_holder = []\n",
    "    av_prec_holder = []\n",
    "    \n",
    "    for fold, (train, test) in enumerate(skf.split(X, y)):\n",
    "        train_pids = user_split_pids.iloc[train].index\n",
    "        test_pids = user_split_pids.iloc[test].index\n",
    "    \n",
    "        X_train = dataset_df_drop.loc[train_pids, feature_column_names].values\n",
    "        X_test = dataset_df_drop.loc[test_pids, feature_column_names].values\n",
    "    \n",
    "        y_train = dataset_df_drop.loc[train_pids, ground_truth_label].values\n",
    "        y_test = dataset_df_drop.loc[test_pids, ground_truth_label].values\n",
    "    \n",
    "        clf = HistGradientBoostingClassifier(random_state = 42, l2_regularization = 0.2, early_stopping = False)\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        y_test_predict_prob = clf.predict_proba(X_test)\n",
    "        y_test_predict = clf.predict(X_test)\n",
    "\n",
    "        weights = calculate_sample_weights(y_test.astype(int))\n",
    "\n",
    "        accuracy_score_holder.append(accuracy_score(y_test, y_test_predict, sample_weight = weights))\n",
    "        au_roc_holder.append(roc_auc_score(y_test, y_test_predict_prob[:,1]))\n",
    "        av_prec_holder.append(average_precision_score(y_test, y_test_predict_prob[:,1]))\n",
    "    \n",
    "    print(f\"Average ROC: {np.array(au_roc_holder).mean().round(3)}, Average Precision: {np.array(av_prec_holder).mean().round(4)}, Average accuracy: {np.array(accuracy_score_holder).mean().round(4)}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c204f174-4e50-463f-a639-71201b3eff19",
   "metadata": {},
   "source": [
    "# COVID-RED Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f35d54fe-04f1-4264-9ffc-dacdaa96f9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Task is is_pos\n",
      "Average ROC: 0.628, Average Precision: 0.0014, Average accuracy: 0.5\n",
      "\n",
      "\n",
      "Task is have_flu\n",
      "Average ROC: 0.657, Average Precision: 0.0306, Average accuracy: 0.4999\n",
      "\n",
      "\n",
      "Task is sx_Fever\n",
      "Average ROC: 0.686, Average Precision: 0.0955, Average accuracy: 0.5091\n",
      "CPU times: user 1min 49s, sys: 4.33 s, total: 1min 53s\n",
      "Wall time: 51.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "optimal_offset = 12\n",
    "optimal_window_length = 10\n",
    "minimum_number = 6\n",
    "prediction_detection = 'prediction'\n",
    "\n",
    "\"\"\"\n",
    "COVID-RED dataset training\n",
    "\"\"\"\n",
    "tasks = ['is_pos', 'have_flu', 'sx_Fever']\n",
    "\n",
    "for ground_truth_label in tasks:\n",
    "    print(\"\\n\")\n",
    "    print(f\"Task is {ground_truth_label}\")\n",
    "\n",
    "    covid_red_cols = [\"WDPULSE\", \"WDRESP\",\"WDTEMP\", \"WDPULSEV\", \"WDOXI\", \"WDSLEEP\"]\n",
    "    \n",
    "    z_scored_df = z_score_modalities(covid_red_nightly, covid_red_cols, optimal_offset, minimum_number, optimal_window_length, 'pid')\n",
    "    z_score_columns = [col + \"_z\" for col in covid_red_cols]\n",
    "    \n",
    "    dataset_df, feature_column_names = get_ML_dataset(z_scored_df, z_score_columns, prediction_detection, 'pid', ground_truth_label)\n",
    "    \n",
    "    dataset_df_drop = dataset_df[np.hstack([feature_column_names, [ground_truth_label]])].replace([np.inf, -np.inf, None], np.nan).dropna()\n",
    "    X, y = dataset_df_drop[feature_column_names].values, dataset_df_drop[ground_truth_label].values\n",
    "    \n",
    "    positive_ids = dataset_df_drop[dataset_df_drop[ground_truth_label] == 1].index.get_level_values(0).unique()\n",
    "    user_split_pids = pd.DataFrame(index = dataset_df_drop.index.get_level_values(0).unique(), columns = [ground_truth_label])\n",
    "    \n",
    "    # Dummy X for input to stratified K fold splitting\n",
    "    X = user_split_pids.values\n",
    "    \n",
    "    user_split_pids[ground_truth_label] = 0\n",
    "    user_split_pids.loc[positive_ids] = 1\n",
    "    \n",
    "    y = user_split_pids.values\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5, random_state = 42, shuffle=True)\n",
    "    skf.get_n_splits(X, y)\n",
    "    \n",
    "    accuracy_score_holder = []\n",
    "    au_roc_holder = []\n",
    "    av_prec_holder = []\n",
    "    \n",
    "    for fold, (train, test) in enumerate(skf.split(X, y)):\n",
    "        train_pids = user_split_pids.iloc[train].index\n",
    "        test_pids = user_split_pids.iloc[test].index\n",
    "    \n",
    "        X_train = dataset_df_drop.loc[train_pids, feature_column_names].values\n",
    "        X_test = dataset_df_drop.loc[test_pids, feature_column_names].values\n",
    "    \n",
    "        y_train = dataset_df_drop.loc[train_pids, ground_truth_label].values\n",
    "        y_test = dataset_df_drop.loc[test_pids, ground_truth_label].values\n",
    "    \n",
    "        clf = HistGradientBoostingClassifier(random_state = 42, l2_regularization = 0.2, early_stopping = False)\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        y_test_predict_prob = clf.predict_proba(X_test)\n",
    "        y_test_predict = clf.predict(X_test)\n",
    "\n",
    "        weights = calculate_sample_weights(y_test.astype(int))\n",
    "\n",
    "        accuracy_score_holder.append(accuracy_score(y_test, y_test_predict, sample_weight = weights))\n",
    "        au_roc_holder.append(roc_auc_score(y_test, y_test_predict_prob[:,1]))\n",
    "        av_prec_holder.append(average_precision_score(y_test, y_test_predict_prob[:,1]))\n",
    "    \n",
    "    print(f\"Average ROC: {np.array(au_roc_holder).mean().round(3)}, Average Precision: {np.array(av_prec_holder).mean().round(4)}, Average accuracy: {np.array(accuracy_score_holder).mean().round(4)}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a760ce28-878c-4d82-bde7-5abf1a9d8487",
   "metadata": {},
   "source": [
    "# COVID-RED Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f65daa3f-2494-4d71-89b3-243979a31486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Task is is_pos\n",
      "Average ROC: 0.638, Average Precision: 0.0041, Average accuracy: 0.5\n",
      "\n",
      "\n",
      "Task is have_flu\n",
      "Average ROC: 0.7, Average Precision: 0.0479, Average accuracy: 0.4999\n",
      "\n",
      "\n",
      "Task is sx_Fever\n",
      "Average ROC: 0.709, Average Precision: 0.0998, Average accuracy: 0.5107\n",
      "CPU times: user 3min 12s, sys: 3.62 s, total: 3min 15s\n",
      "Wall time: 33.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "optimal_offset = 12\n",
    "optimal_window_length = 10\n",
    "minimum_number = 6\n",
    "prediction_detection = 'detection'\n",
    "\n",
    "\"\"\"\n",
    "COVID-RED dataset training\n",
    "\"\"\"\n",
    "tasks = ['is_pos', 'have_flu', 'sx_Fever']\n",
    "\n",
    "for ground_truth_label in tasks:\n",
    "    print(\"\\n\")\n",
    "    print(f\"Task is {ground_truth_label}\")\n",
    "\n",
    "    covid_red_cols = [\"WDPULSE\", \"WDRESP\",\"WDTEMP\", \"WDPULSEV\", \"WDOXI\", \"WDSLEEP\"]\n",
    "    \n",
    "    z_scored_df = z_score_modalities(covid_red_nightly, covid_red_cols, optimal_offset, minimum_number, optimal_window_length, 'pid')\n",
    "    z_score_columns = [col + \"_z\" for col in covid_red_cols]\n",
    "    \n",
    "    dataset_df, feature_column_names = get_ML_dataset(z_scored_df, z_score_columns, prediction_detection, 'pid', ground_truth_label)\n",
    "    \n",
    "    dataset_df_drop = dataset_df[np.hstack([feature_column_names, [ground_truth_label]])].replace([np.inf, -np.inf, None], np.nan).dropna()\n",
    "    X, y = dataset_df_drop[feature_column_names].values, dataset_df_drop[ground_truth_label].values\n",
    "    \n",
    "    positive_ids = dataset_df_drop[dataset_df_drop[ground_truth_label] == 1].index.get_level_values(0).unique()\n",
    "    user_split_pids = pd.DataFrame(index = dataset_df_drop.index.get_level_values(0).unique(), columns = [ground_truth_label])\n",
    "    \n",
    "    # Dummy X for input to stratified K fold splitting\n",
    "    X = user_split_pids.values\n",
    "    \n",
    "    user_split_pids[ground_truth_label] = 0\n",
    "    user_split_pids.loc[positive_ids] = 1\n",
    "    \n",
    "    y = user_split_pids.values\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5, random_state = 42, shuffle=True)\n",
    "    skf.get_n_splits(X, y)\n",
    "    \n",
    "    accuracy_score_holder = []\n",
    "    au_roc_holder = []\n",
    "    av_prec_holder = []\n",
    "    \n",
    "    for fold, (train, test) in enumerate(skf.split(X, y)):\n",
    "        train_pids = user_split_pids.iloc[train].index\n",
    "        test_pids = user_split_pids.iloc[test].index\n",
    "    \n",
    "        X_train = dataset_df_drop.loc[train_pids, feature_column_names].values\n",
    "        X_test = dataset_df_drop.loc[test_pids, feature_column_names].values\n",
    "    \n",
    "        y_train = dataset_df_drop.loc[train_pids, ground_truth_label].values\n",
    "        y_test = dataset_df_drop.loc[test_pids, ground_truth_label].values\n",
    "    \n",
    "        clf = HistGradientBoostingClassifier(random_state = 42, l2_regularization = 0.2, early_stopping = False)\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        y_test_predict_prob = clf.predict_proba(X_test)\n",
    "        y_test_predict = clf.predict(X_test)\n",
    "\n",
    "        weights = calculate_sample_weights(y_test.astype(int))\n",
    "\n",
    "        accuracy_score_holder.append(accuracy_score(y_test, y_test_predict, sample_weight = weights))\n",
    "        au_roc_holder.append(roc_auc_score(y_test, y_test_predict_prob[:,1]))\n",
    "        av_prec_holder.append(average_precision_score(y_test, y_test_predict_prob[:,1]))\n",
    "    \n",
    "    print(f\"Average ROC: {np.array(au_roc_holder).mean().round(3)}, Average Precision: {np.array(av_prec_holder).mean().round(4)}, Average accuracy: {np.array(accuracy_score_holder).mean().round(4)}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0666dcc8-17b7-4ebc-9ad8-ef719e9b0a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "prediction_detection = 'prediction'\n",
    "ground_truth_label = 'sx_Fever'\n",
    "#ground_truth_label = 'is_pos'\n",
    "#ground_truth_label = 'have_flu'\n",
    "\n",
    "optimal_offset = 12\n",
    "optimal_window_length = 10\n",
    "minimum_number = 6\n",
    "\n",
    "\"\"\"\n",
    "COVID-RED dataset\n",
    "\"\"\"\n",
    "covid_red_generalization_columns = [\"WDPULSE\", \"WDSLEEP\"]\n",
    "z_scored_df = z_score_modalities(covid_red_df, covid_red_generalization_columns, optimal_offset, minimum_number, optimal_window_length, 'SUBJID')\n",
    "z_score_columns = [col + \"_z\" for col in covid_red_generalization_columns]\n",
    "dataset_df, feature_column_names = get_ML_dataset(z_scored_df, z_score_columns, 'prediction', 'SUBJID', 'sx_Fever')\n",
    "dataset_df[['is_pos', 'have_flu']] = covid_red_df[['is_pos', 'have_flu']].values\n",
    "covid_red_df_drop = dataset_df[np.hstack([feature_column_names, ['sx_Fever', 'is_pos', 'have_flu']])].replace([np.inf, -np.inf, None], np.nan).dropna()\n",
    "\n",
    "\"\"\"\n",
    "Homekit dataset\n",
    "\"\"\"\n",
    "homekit_generalization_columns = ['resting_heart_rate','total_asleep_minutes']\n",
    "z_scored_df = z_score_modalities(homekit_nightly, homekit_generalization_columns, optimal_offset, minimum_number, optimal_window_length, 'participant_id')\n",
    "z_score_columns = [col + \"_z\" for col in homekit_generalization_columns]\n",
    "dataset_df, feature_column_names = get_ML_dataset(z_scored_df, z_score_columns, 'prediction', 'participant_id', 'sx_Fever')\n",
    "dataset_df[['is_pos', 'have_flu']] = homekit_nightly[['is_pos', 'have_flu']].values\n",
    "homekit_df_drop = dataset_df[np.hstack([feature_column_names, ['sx_Fever', 'is_pos', 'have_flu']])].replace([np.inf, -np.inf, None], np.nan).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b936cb9b-a6ec-460f-9a81-6136d8507aaa",
   "metadata": {},
   "source": [
    "# Homekit -> COVID-RED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f1a4b7-9dad-419f-9511-6367de06e0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "optimal_offset = 12\n",
    "optimal_window_length = 10\n",
    "minimum_number = 6\n",
    "\n",
    "\"\"\"\n",
    "Homekit dataset training\n",
    "\"\"\"\n",
    "tasks = ['is_pos', 'have_flu', 'sx_Fever']\n",
    "prediction_detection = 'prediction'\n",
    "\n",
    "for ground_truth_label in tasks:\n",
    "    print(\"\\n\")\n",
    "    print(f\"Task is {ground_truth_label}\")\n",
    "\n",
    "    \"\"\"\n",
    "    Homekit generalization testing\n",
    "    \"\"\"\n",
    "    homekit_generalization_columns = ['resting_heart_rate','total_asleep_minutes']\n",
    "    z_scored_df = z_score_modalities(homekit_nightly, homekit_generalization_columns, optimal_offset, minimum_number, optimal_window_length, 'pid')\n",
    "    z_score_columns = [col + \"_z\" for col in homekit_generalization_columns]\n",
    "    dataset_df, homekit_feature_column_names = get_ML_dataset(z_scored_df, z_score_columns, prediction_detection, 'pid', ground_truth_label)\n",
    "    homekit_dataset_df_drop = dataset_df[np.hstack([homekit_feature_column_names, [ground_truth_label]])].replace([np.inf, -np.inf, None], np.nan).dropna()\n",
    "    X, y = homekit_dataset_df_drop[homekit_feature_column_names].values, homekit_dataset_df_drop[ground_truth_label].values    \n",
    "    \n",
    "    positive_ids = homekit_dataset_df_drop[homekit_dataset_df_drop[ground_truth_label] == 1].index.get_level_values(0).unique()\n",
    "    user_split_pids = pd.DataFrame(index = homekit_dataset_df_drop.index.get_level_values(0).unique(), columns = [ground_truth_label])\n",
    "    \n",
    "    # Dummy X for input to stratified K fold splitting\n",
    "    X = user_split_pids.values\n",
    "    \n",
    "    user_split_pids[ground_truth_label] = 0\n",
    "    user_split_pids.loc[positive_ids] = 1\n",
    "    \n",
    "    y = user_split_pids.values\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5, random_state = 42, shuffle=True)\n",
    "    skf.get_n_splits(X, y)\n",
    "    \n",
    "    f1_score_holder = []\n",
    "    au_roc_holder = []\n",
    "    av_prec_holder = []\n",
    "    concept_shift_holder_covid_red = []\n",
    "    #concept_shift_holder_homekit = []\n",
    "\n",
    "    \"\"\"\n",
    "    COVID-RED dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    covid_red_generalization_columns = [\"WDPULSE\", \"WDSLEEP\"]\n",
    "    \n",
    "    z_scored_df = z_score_modalities(covid_red_nightly, covid_red_generalization_columns, optimal_offset, minimum_number, optimal_window_length, 'pid')\n",
    "    z_score_columns = [col + \"_z\" for col in covid_red_generalization_columns]\n",
    "    \n",
    "    dataset_df, covid_red_feature_column_names = get_ML_dataset(z_scored_df, z_score_columns, prediction_detection, 'pid', ground_truth_label)\n",
    "    \n",
    "    covid_red_dataset_df_drop = dataset_df[np.hstack([covid_red_feature_column_names, [ground_truth_label]])].replace([np.inf, -np.inf, None], np.nan).dropna()\n",
    "    covid_red_X, covid_red_y = covid_red_dataset_df_drop[covid_red_feature_column_names].values, covid_red_dataset_df_drop[ground_truth_label].values\n",
    "\n",
    "\n",
    "    for fold, (train, test) in enumerate(skf.split(X, y)):\n",
    "        train_pids = user_split_pids.iloc[train].index\n",
    "        test_pids = user_split_pids.iloc[test].index\n",
    "    \n",
    "        X_train = homekit_dataset_df_drop.loc[train_pids, homekit_feature_column_names].values\n",
    "        X_test = homekit_dataset_df_drop.loc[test_pids, homekit_feature_column_names].values\n",
    "    \n",
    "        y_train = homekit_dataset_df_drop.loc[train_pids, ground_truth_label].values\n",
    "        y_test = homekit_dataset_df_drop.loc[test_pids, ground_truth_label].values\n",
    "    \n",
    "        clf = HistGradientBoostingClassifier(random_state = 42, l2_regularization = 0.2, early_stopping = False)\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        y_test_predict_prob = clf.predict_proba(X_test)\n",
    "        y_test_predict = clf.predict(X_test)\n",
    "        \n",
    "        au_roc_holder.append(roc_auc_score(y_test, y_test_predict_prob[:,1]))\n",
    "        av_prec_holder.append(average_precision_score(y_test, y_test_predict_prob[:,1]))\n",
    "\n",
    "        \"\"\"\n",
    "        Determine proportion of shift due to P(Y|X) shift\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        Concept shift: COVID-RED\n",
    "        \"\"\"\n",
    "        model = copy.deepcopy(clf)\n",
    "        target_X, target_y = covid_red_X, covid_red_y\n",
    "        source_X, source_y = X_test, y_test\n",
    "    \n",
    "        p2p, q2q, p2s, s2q = degradation_decomp(source_X, source_y, target_X, target_y, model, data_sum=20000, K=8, draw_calibration=False)\n",
    "        concept_shift_holder_covid_red.append(np.abs(p2s-s2q)/np.abs(p2p-q2q))\n",
    "\n",
    "\n",
    "        \n",
    "    print(f\"Average ROC: {np.array(au_roc_holder).mean().round(3)}, Average Precision: {np.array(av_prec_holder).mean().round(4)}\")\n",
    "    print(f\"Average proportion of concept shift. COVID-RED: {np.array(concept_shift_holder_covid_red).mean().round(3)}, \")\n",
    "\n",
    "    clf = HistGradientBoostingClassifier(random_state = 42, l2_regularization = 0.2, early_stopping = False)\n",
    "    clf.fit(homekit_dataset_df_drop[homekit_feature_column_names].values, homekit_dataset_df_drop[ground_truth_label].values)\n",
    "    \n",
    "    \"\"\"\n",
    "    COVID-RED predictions\n",
    "    \"\"\"\n",
    "    y_test_predict_prob = clf.predict_proba(covid_red_X)\n",
    "    y_test_predict = clf.predict(covid_red_X)\n",
    "    \n",
    "    print('\\n')\n",
    "    #print(f\"F1 score, {f1_score(covid_red_y, y_test_predict)}\")\n",
    "    print(f\"Average precision, {average_precision_score(covid_red_y, y_test_predict_prob[:,1]).round(4)}\")\n",
    "    print(f\"AUC ROC, {roc_auc_score(covid_red_y, y_test_predict_prob[:,1]).round(3)}\")\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83589c29",
   "metadata": {},
   "source": [
    "# COVID-RED -> Homekit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239d52d2-a18b-4578-8fab-1a8acfb74b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "optimal_offset = 12\n",
    "optimal_window_length = 10\n",
    "minimum_number = 6\n",
    "\n",
    "\"\"\"\n",
    "COVID-RED dataset training\n",
    "\"\"\"\n",
    "tasks = ['is_pos', 'have_flu', 'sx_Fever']\n",
    "prediction_detection = 'prediction'\n",
    "\n",
    "for ground_truth_label in tasks:\n",
    "    print(\"\\n\")\n",
    "    print(f\"Task is {ground_truth_label}\")\n",
    "\n",
    "    \"\"\"\n",
    "    COVID-RED dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    covid_red_generalization_columns = [\"WDPULSE\", \"WDSLEEP\"]\n",
    "    \n",
    "    z_scored_df = z_score_modalities(covid_red_nightly, covid_red_generalization_columns, optimal_offset, minimum_number, optimal_window_length, 'pid')\n",
    "    z_score_columns = [col + \"_z\" for col in covid_red_generalization_columns]\n",
    "    dataset_df, covid_red_feature_column_names = get_ML_dataset(z_scored_df, z_score_columns, prediction_detection, 'pid', ground_truth_label)\n",
    "    covid_red_dataset_df_drop = dataset_df[np.hstack([covid_red_feature_column_names, [ground_truth_label]])].replace([np.inf, -np.inf, None], np.nan).dropna()\n",
    "    \n",
    "    X, y = covid_red_dataset_df_drop[covid_red_feature_column_names].values, covid_red_dataset_df_drop[ground_truth_label].values\n",
    "    \n",
    "    positive_ids = covid_red_dataset_df_drop[covid_red_dataset_df_drop[ground_truth_label] == 1].index.get_level_values(0).unique()\n",
    "    user_split_pids = pd.DataFrame(index = covid_red_dataset_df_drop.index.get_level_values(0).unique(), columns = [ground_truth_label])\n",
    "    \n",
    "    # Dummy X for input to stratified K fold splitting\n",
    "    X = user_split_pids.values\n",
    "    \n",
    "    user_split_pids[ground_truth_label] = 0\n",
    "    user_split_pids.loc[positive_ids] = 1\n",
    "    \n",
    "    y = user_split_pids.values\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5, random_state = 42, shuffle=True)\n",
    "    skf.get_n_splits(X, y)\n",
    "    \n",
    "    f1_score_holder = []\n",
    "    au_roc_holder = []\n",
    "    av_prec_holder = []\n",
    "    #concept_shift_holder_covid_red = []\n",
    "    concept_shift_holder_homekit = []\n",
    "\n",
    "    \"\"\"\n",
    "    Homekit generalization testing\n",
    "    \"\"\"\n",
    "    homekit_generalization_columns = ['resting_heart_rate','total_asleep_minutes']\n",
    "    z_scored_df = z_score_modalities(homekit_nightly, homekit_generalization_columns, optimal_offset, minimum_number, optimal_window_length, 'pid')\n",
    "    z_score_columns = [col + \"_z\" for col in homekit_generalization_columns]\n",
    "    dataset_df, homekit_feature_column_names = get_ML_dataset(z_scored_df, z_score_columns, prediction_detection, 'pid', ground_truth_label)\n",
    "    homekit_dataset_df_drop = dataset_df[np.hstack([homekit_feature_column_names, [ground_truth_label]])].replace([np.inf, -np.inf, None], np.nan).dropna()\n",
    "    homekit_X, homekit_y = homekit_dataset_df_drop[homekit_feature_column_names].values, homekit_dataset_df_drop[ground_truth_label].values\n",
    "\n",
    "    \n",
    "    for fold, (train, test) in enumerate(skf.split(X, y)):\n",
    "        train_pids = user_split_pids.iloc[train].index\n",
    "        test_pids = user_split_pids.iloc[test].index\n",
    "    \n",
    "        X_train = covid_red_dataset_df_drop.loc[train_pids, covid_red_feature_column_names].values\n",
    "        X_test = covid_red_dataset_df_drop.loc[test_pids, covid_red_feature_column_names].values\n",
    "    \n",
    "        y_train = covid_red_dataset_df_drop.loc[train_pids, ground_truth_label].values\n",
    "        y_test = covid_red_dataset_df_drop.loc[test_pids, ground_truth_label].values\n",
    "    \n",
    "        clf = HistGradientBoostingClassifier(random_state = 42, l2_regularization = 0.2, early_stopping = False)\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        y_test_predict_prob = clf.predict_proba(X_test)\n",
    "        y_test_predict = clf.predict(X_test)\n",
    "        \n",
    "        au_roc_holder.append(roc_auc_score(y_test, y_test_predict_prob[:,1]))\n",
    "        av_prec_holder.append(average_precision_score(y_test, y_test_predict_prob[:,1]))\n",
    "\n",
    "        \"\"\"\n",
    "        Determine proportion of shift due to P(Y|X) shift\n",
    "        \"\"\"\n",
    "    \n",
    "        \"\"\"\n",
    "        Concept shift: Homekit\n",
    "        \"\"\"\n",
    "        model = copy.deepcopy(clf)\n",
    "        target_X, target_y = homekit_X, homekit_y\n",
    "        source_X, source_y = X_test, y_test\n",
    "    \n",
    "        p2p, q2q, p2s, s2q = degradation_decomp(source_X, source_y, target_X, target_y, model, data_sum=20000, K=8, draw_calibration=False)\n",
    "        concept_shift_holder_homekit.append(np.abs(p2s-s2q)/np.abs(p2p-q2q))\n",
    "        \n",
    "\n",
    "        \n",
    "    print(f\"Average ROC: {np.array(au_roc_holder).mean().round(3)}, Average Precision: {np.array(av_prec_holder).mean().round(4)}\")\n",
    "    print(f\"Average proportion of concept shift. Homekit: {np.array(concept_shift_holder_homekit).mean().round(3)}, \")\n",
    "\n",
    "    clf = HistGradientBoostingClassifier(random_state = 42, l2_regularization = 0.2, early_stopping = False)\n",
    "    clf.fit(covid_red_dataset_df_drop[covid_red_feature_column_names].values, covid_red_dataset_df_drop[ground_truth_label].values)\n",
    "    \n",
    "    \"\"\"\n",
    "    Homekit prediction\n",
    "    \"\"\"\n",
    "    y_test_predict_prob = clf.predict_proba(homekit_X)\n",
    "    y_test_predict = clf.predict(homekit_X)\n",
    "    \n",
    "    print('\\n')\n",
    "    print(\"HGBC COVID-RED -> Homekit\")\n",
    "    #print(f\"F1 score, {f1_score(homekit_y, y_test_predict)}\")\n",
    "    print(f\"Average precision, {average_precision_score(homekit_y, y_test_predict_prob[:,1]).round(3)}\")\n",
    "    print(f\"AUC ROC, {roc_auc_score(homekit_y, y_test_predict_prob[:,1]).round(3)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
